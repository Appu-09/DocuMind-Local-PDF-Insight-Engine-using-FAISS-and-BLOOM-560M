# -*- coding: utf-8 -*-
"""DocuMind.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12QB_d1ydeRIH3EfkcE4m1I_dEkHXZ1qV
"""

!pip install PyPDF2 sentence-transformers faiss-cpu langchain

from google.colab import files

uploaded = files.upload()

from PyPDF2 import PdfReader

def load_pdf_text(file_path):
    pdf = PdfReader(file_path)
    text = ""
    for page in pdf.pages:
        text += page.extract_text() + "\n"
    return text

# Replace 'your_document.pdf' with the actual uploaded filename
pdf_filename = list(uploaded.keys())[0]
pdf_text = load_pdf_text(pdf_filename)
print(pdf_text[:500])  # print first 500 characters to check

from langchain.text_splitter import CharacterTextSplitter

# Split text into smaller chunks (e.g., 1000 characters each with 200 overlap)
text_splitter = CharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)

chunks = text_splitter.split_text(pdf_text)
print(f"Total chunks created: {len(chunks)}")
print(chunks[:2])  # print first 2 chunks to check

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

# Load a pre-trained embedding model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Generate embeddings for each chunk
embeddings = model.encode(chunks)

# Convert embeddings to float32 for FAISS
embeddings = np.array(embeddings).astype("float32")

# Create FAISS index
dimension = embeddings.shape[1]  # embedding size
index = faiss.IndexFlatL2(dimension)
index.add(embeddings)

print(f"FAISS index has {index.ntotal} vectors.")

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

# 1️⃣ Load the sentence transformer model
embedder = SentenceTransformer('all-MiniLM-L6-v2')  # lightweight & good for semantic search

# 2️⃣ Split your PDF text into chunks
def chunk_text(text, chunk_size=500):
    chunks = []
    for i in range(0, len(text), chunk_size):
        chunks.append(text[i:i+chunk_size])
    return chunks

pdf_chunks = chunk_text(pdf_text)
print(f"Number of vectors in FAISS index: {index.ntotal}")

query = "What is the main topic of the document?"
top_chunks = search_faiss(query, k=3)

for i, chunk in enumerate(top_chunks, 1):
    print(f"--- Chunk {i} ---\n{chunk}\n")

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# Load model and tokenizer
model_name = "bigscience/bloom-560m"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    dtype="auto",            # use optimal precision
    low_cpu_mem_usage=True
)

# CPU pipeline
qa_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device=-1  # CPU
)

def answer_question_local(query, top_chunks, max_tokens=100):
    # Use only first 250-300 chars of each chunk
    context = "\n\n".join([chunk[:300].replace("\n", " ") for chunk in top_chunks])

    prompt = (
        f"Answer the question concisely in 1-2 sentences using only the context below. "
        f"Do not add extra information or repeat the context.\n\n"
        f"Context:\n{context}\n\nQuestion: {query}\nAnswer:"
    )

    response = qa_pipeline(
        prompt,
        max_new_tokens=max_tokens,
        do_sample=False,        # deterministic output
        repetition_penalty=2.0
    )

    # Clean up the generated text
    answer_text = response[0]['generated_text']
    # Remove everything before the answer
    answer = answer_text.split("Answer:")[-1].strip()
    # Stop at any extra questions or colons to avoid verbose continuation
    answer = answer.split("Question")[0].split("A:")[0].strip()
    return answer

# Example usage
query = "What teaching method did the master use with students?"
top_chunks = search_faiss(query, k=3)
answer = answer_question_local(query, top_chunks)
print(answer)